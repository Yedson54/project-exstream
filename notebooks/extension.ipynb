{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stream Processing - Project 3 : Ideas for Extension (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies.\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files\n",
    "DATAPATH =  \"../data/custom_no_streaming_8/folder_2\"\n",
    "files = glob.glob(rf\"{DATAPATH}/*\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read anomaly files and concatenate dataframes\n",
    "dfs = []\n",
    "for file in files[:-1]:\n",
    "    df = pd.read_csv(file)\n",
    "    # Extract the filename without the parent folder path and extension\n",
    "    filename = file.split('\\\\')[-1].split('.')[0]\n",
    "    # Add a new column \"anomaly_type\" with the extracted filename\n",
    "    df.insert(1, column=\"trace_id\", value=filename)\n",
    "    df.rename({\"Unnamed: 0\": \"time\"}, axis=1, inplace=True)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes except labels.csv\n",
    "anomaly_df = pd.concat(dfs)\n",
    "anomaly_df\n",
    "\n",
    "# Rename values in the \"anomaly_type\" column based on the specified mapping\n",
    "mapping = {\n",
    "    \"1_3\": \"bursty_input\",\n",
    "    \"1_4\": \"bursty_input\",\n",
    "    \"2_3\": \"stalled_input\",\n",
    "    \"2_4\": \"stalled_input\",\n",
    "    \"3_3\": \"cpu_contention\",\n",
    "    \"3_4\": \"cpu_contention\"\n",
    "}\n",
    "anomaly_df.insert(2, column=\"anomaly_type\", value=anomaly_df['trace_id'].replace(mapping))\n",
    "\n",
    "cols = ['driver_jvm_heap_used_value', 'avg_jvm_heap_used_value',\n",
    "       'avg_executor_filesystem_hdfs_write_ops_value_1_diff',\n",
    "       'avg_executor_cpuTime_count_1_diff',\n",
    "       'avg_executor_runTime_count_1_diff',\n",
    "       'avg_executor_shuffleRecordsRead_count_1_diff',\n",
    "       'avg_executor_shuffleRecordsWritten_count_1_diff']\n",
    "features = len(cols)\n",
    "\n",
    "\n",
    "anomaly_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def anomaly_detection(anomaly_df, trace_id, layers=10, epochs=20, batch_size=128, validation_split=0.2, shuffle=True, learning_rate=0.001, threshold=95):\n",
    "    \"\"\"\n",
    "    function to detect anomalies in the input data\n",
    "    :param anomaly_df: input dataframe\n",
    "    :return: dataframe with detected anomalies\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Charger les données\n",
    "    # Assurez-vous que votre fichier CSV est correctement formaté et accessible\n",
    "    df = anomaly_df[anomaly_df['trace_id'] == '1_3'].copy()\n",
    "\n",
    "    # Sélectionner les colonnes numériques pertinentes (exclure 'time' et 'trace_id')\n",
    "    cols = df.columns[3:]\n",
    "\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[cols])\n",
    "\n",
    "    # Reshape data pour LSTM [samples, time steps, features]\n",
    "    timesteps = 1  # Vous pouvez ajuster cela selon votre séquence de temps\n",
    "    features = len(cols)\n",
    "    data = df_scaled.reshape(df_scaled.shape[0], timesteps, features)\n",
    "\n",
    "    # Construction de l'autoencodeur LSTM\n",
    "    inputs = Input(shape=(timesteps, features))\n",
    "    encoded = LSTM(10, return_sequences=False)(inputs)\n",
    "    decoded = RepeatVector(timesteps)(encoded)\n",
    "    decoded = LSTM(features, return_sequences=True)(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    autoencoder.fit(data, data, epochs=20, batch_size=128, validation_split=0.2, shuffle=True)\n",
    "\n",
    "    # Prédire les données reconstruites\n",
    "    predictions = autoencoder.predict(data)\n",
    "\n",
    "    # Calculer l'erreur de reconstruction\n",
    "    mse = np.mean(np.power(data - predictions, 2), axis=1)\n",
    "    error_df = pd.DataFrame({'Reconstruction_error': mse[:,0]})\n",
    "\n",
    "    # Identifier les anomalies (Vous pouvez définir un seuil d'erreur)\n",
    "    threshold = np.percentile(error_df.Reconstruction_error.values, 95)\n",
    "    anomalies = error_df[error_df.Reconstruction_error > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_detection(anomaly_df, \n",
    "                      trace_id : str, \n",
    "                      layers : int = 10, \n",
    "                      epochs : int = 20, \n",
    "                      batch_size : int = 128, \n",
    "                      validation_split : float = 0.2, \n",
    "                      shuffle : bool = True, \n",
    "                      learning_rate=0.001, \n",
    "                      threshold_percentile : int = 95, \n",
    "                      verbose : bool = False):\n",
    "    \"\"\"\n",
    "    Function to detect anomalies in the input data.\n",
    "    :param anomaly_df: Input DataFrame containing the time series data.\n",
    "    :param trace_id: Trace ID to filter the data.\n",
    "    :param layers: Number of neurons in LSTM layers.\n",
    "    :param epochs: Number of epochs for training the autoencoder.\n",
    "    :param batch_size: Batch size for training.\n",
    "    :param validation_split: Fraction of data to be used for validation.\n",
    "    :param shuffle: Whether to shuffle data before each epoch.\n",
    "    :param learning_rate: Learning rate for the optimizer.\n",
    "    :param threshold_percentile: Percentile to use for thresholding anomalies.\n",
    "    :return: DataFrame containing the detected anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter data for the given trace_id\n",
    "    df = anomaly_df[anomaly_df['trace_id'] == trace_id].copy()\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[cols])\n",
    "\n",
    "    # Reshape data for LSTM [samples, time steps, features]\n",
    "    timesteps = 1\n",
    "    data = df_scaled.reshape(df_scaled.shape[0], timesteps, features)\n",
    "\n",
    "    # Construct LSTM autoencoder\n",
    "    inputs = Input(shape=(timesteps, features))\n",
    "    encoded = LSTM(layers, return_sequences=False)(inputs)\n",
    "    decoded = RepeatVector(timesteps)(encoded)\n",
    "    decoded = LSTM(features, return_sequences=True)(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    autoencoder.fit(data, data, epochs=epochs, batch_size=batch_size, validation_split=validation_split, shuffle=shuffle, verbose=verbose)\n",
    "\n",
    "    # Predict the reconstructed data\n",
    "    predictions = autoencoder.predict(data)\n",
    "\n",
    "    # Calculate reconstruction error\n",
    "    mse = np.mean(np.power(data - predictions, 2), axis=1)\n",
    "    error_df = pd.DataFrame({'Reconstruction_error': mse[:, 0], 'time': df['time']})\n",
    "\n",
    "    # Identify anomalies (setting a threshold for error)\n",
    "    threshold = np.percentile(error_df.Reconstruction_error.values, threshold_percentile)\n",
    "    anomalies = error_df[error_df.Reconstruction_error > threshold]\n",
    "\n",
    "    return anomalies, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies, predictions = anomaly_detection(anomaly_df, '3_3', layers=10, epochs=20, batch_size=128, validation_split=0.2, shuffle=True, learning_rate=0.001, threshold_percentile=95, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(df, predictions, cols):\n",
    "    \"\"\"\n",
    "    Plot the original and reconstructed time series data.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The original dataframe.\n",
    "        predictions : np.array\n",
    "        The reconstructed data.\n",
    "    cols : list\n",
    "\n",
    "    scaler : StandardScaler\n",
    "        The scaler used to scale the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.            \n",
    "    \"\"\"\n",
    "    # normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[cols])\n",
    "    \n",
    "    # Rescale predictions back to original scale\n",
    "    predictions_rescaled = scaler.inverse_transform(predictions.reshape(predictions.shape[0], features))\n",
    "\n",
    "    # Set Seaborn style\n",
    "    sns.set(style='whitegrid')\n",
    "\n",
    "    # Number of variables (excluding 'time' and 'trace_id')\n",
    "    num_vars = len(cols)\n",
    "\n",
    "    # Create subplots\n",
    "    # add 3 columns for the time, trace_id, and anomaly_type columns\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(num_vars, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    for i, col in enumerate(cols):\n",
    "        # Original Data\n",
    "        axes[i].plot(df['time'], df[col], label='Original', color='blue', linewidth=1)\n",
    "\n",
    "        # Reconstructed Data\n",
    "        axes[i].plot(df['time'], predictions_rescaled[:, i], label='Reconstructed', color='orange', linewidth=1)\n",
    "\n",
    "        # Titles and Labels\n",
    "        axes[i].set_title(f'Reconstruction Comparison for {col}')\n",
    "        #axes[i].set_ylabel(col)\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Set common X label\n",
    "    axes[-1].set_xlabel('Time')\n",
    "\n",
    "    # Adjust layout for better readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction(df, predictions, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(df, anomalies, cols):\n",
    "    \"\"\"\n",
    "    Fonction pour tracer les anomalies détectées par l'autoencodeur LSTM.\n",
    "    Paramètres:\n",
    "        df (DataFrame): DataFrame contenant les données d'origine\n",
    "        anomalies (DataFrame): DataFrame contenant les anomalies détectées\n",
    "        cols (list): Liste des colonnes à tracer\n",
    "    \"\"\"\n",
    "    # Ajouter la colonne 'time' pour référence temporelle\n",
    "    anomalies['time'] = anomalies.index\n",
    "\n",
    "    # Set the Seaborn style for better aesthetics\n",
    "    sns.set(style='whitegrid')\n",
    "\n",
    "    # Define color palette for the time series\n",
    "    colors = sns.color_palette('tab10', len(cols))\n",
    "\n",
    "    # Create a figure and axis for the plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Plot each time series with a thinner line\n",
    "    for i, col in enumerate(cols):\n",
    "        ax.plot(df['time'], df[col], color=colors[i], linewidth=1, label=col)\n",
    "\n",
    "    # Highlight anomaly points with red color\n",
    "    ax.scatter(anomalies['time'], df.loc[anomalies.index, cols[0]], color='red', s=50, label='Anomalies', zorder=5)\n",
    "\n",
    "    # Set title and labels with appropriate font sizes\n",
    "    ax.set_title('Time Series with Anomaly Points', fontsize=16)\n",
    "    ax.set_xlabel('Time', fontsize=14)\n",
    "    ax.set_ylabel('Normalized Values', fontsize=14)\n",
    "\n",
    "    # Place the legend in the upper right corner of the plot\n",
    "    ax.legend(loc='upper right', frameon=True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(df, anomalies, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testify_continuity(anomalies):\n",
    "    \"\"\"\n",
    "    Function to testify the continuity of anomalies.\n",
    "    Parameters:\n",
    "        anomalies (DataFrame): DataFrame containing the detected anomalies\n",
    "    \"\"\"\n",
    "    # trier et ranger du plus petit au plus grand\n",
    "    anomalies = anomalies.sort_values(by=['time'])\n",
    "    diff = [j-i for i, j in zip(anomalies['time'][:-1], anomalies['time'][1:])]\n",
    "\n",
    "    # print the max diff between anomalies\n",
    "    max(diff)\n",
    "\n",
    "    # plot the distribution of the time difference between anomalies\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.hist(diff, bins=20)\n",
    "    plt.title('Distribution of Time Difference Between Anomalies')\n",
    "    plt.xlabel('Time Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testify_continuity(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "traces = anomaly_df['trace_id'].unique().tolist()\n",
    "\n",
    "# load results\n",
    "results_to_fill = pd.read_csv('../data/custom_no_streaming_8/folder_2_results.csv', index_col=0)\n",
    "\n",
    "for trace_id in traces:\n",
    "    # Détecter les anomalies pour chaque trace_id\n",
    "    anomalies = anomaly_detection(anomaly_df, trace_id, layers=10, epochs=20, batch_size=128, validation_split=0.2, shuffle=True, learning_rate=0.001, threshold_percentile=95)\n",
    "\n",
    "    # Récupérer le min et le max du time dans anomalies pour chaque trace_id\n",
    "    if not anomalies.empty:\n",
    "        min = anomalies['time'].min()\n",
    "        max = anomalies['time'].max()\n",
    "    else:\n",
    "        min = np.nan\n",
    "        max = np.nan\n",
    "    \n",
    "    results_list.append({\n",
    "        'ano_start': min,\n",
    "        'ano_end': max,\n",
    "        'trace_id': trace_id\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "\n",
    "# Faire une jointure entre anomalies et results\n",
    "final_results = pd.merge(results_df, results_to_fill, on='trace_id', how='left')\n",
    "\n",
    "# Afficher les résultats finaux\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
