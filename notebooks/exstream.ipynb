{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stream Processing - Project 3 : EXstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies.\n",
    "import glob\n",
    "import warnings\n",
    "from itertools import groupby\n",
    "from typing import List, Dict, Tuple, Optional, Literal, Union, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files\n",
    "DATAPATH =  \"../data/custom_no_streaming_8/folder_1\"\n",
    "files = glob.glob(rf\"{DATAPATH}/*\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read anomaly files and concatenate dataframes\n",
    "dfs = []\n",
    "for file in files[:-1]:\n",
    "    df = pd.read_csv(file)\n",
    "    # Extract the filename without the parent folder path and extension\n",
    "    filename = file.split('\\\\')[-1].split('.')[0]\n",
    "    # Add a new column \"anomaly_type\" with the extracted filename\n",
    "    df.insert(1, column=\"trace_id\", value=filename)\n",
    "    df.rename({\"Unnamed: 0\": \"time\"}, axis=1, inplace=True)\n",
    "    dfs.append(df)\n",
    "# Concatenate all dataframes except labels.csv\n",
    "anomaly_df = pd.concat(dfs)\n",
    "\n",
    "# Create a separate dataframe for labels.csv\n",
    "labels_df = pd.read_csv(files[-1], index_col=0)\n",
    "\n",
    "# Print the resulting dataframes\n",
    "print(\"Concatenated DataFrame (all but 'labels.csv'):\\n\", anomaly_df.shape)\n",
    "print(\"\\nLabels DataFrame:\\n\", labels_df.shape)\n",
    "\n",
    "\n",
    "# # Rename values in the \"anomaly_type\" column based on the specified mapping\n",
    "# mapping = {\n",
    "#     \"1_1\": \"bursty_input\",\n",
    "#     \"1_2\": \"bursty_input\",\n",
    "#     \"2_1\": \"stalled_input\",\n",
    "#     \"2_2\": \"stalled_input\",\n",
    "#     \"3_1\": \"cpu_contention\",\n",
    "#     \"3_2\": \"cpu_contention\"\n",
    "# }\n",
    "# anomaly_df.insert(2, column=\"anomaly_type\", value=anomaly_df['trace_id'].replace(mapping))\n",
    "\n",
    "# Missing values.\n",
    "print(\"\\nTotal no. missing values:\\n\", anomaly_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_ano_time(x: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame based on time intervals.\n",
    "\n",
    "    Parameters:\n",
    "    x (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A boolean array indicating whether each row satisfies the time intervals.\n",
    "    \"\"\"\n",
    "    return np.logical_or(x['time'].between(x['ref_start'], x['ref_end']),\n",
    "                         x['time'].between(x['ano_start'], x['ano_end']))\n",
    "\n",
    "# Merge anomaly_df and labels and filter based on time.\n",
    "# Keep only point observed on annotated period. (#TODO: REMIND THIS FOR LATER)\n",
    "anomaly_df = anomaly_df.merge(\n",
    "    labels_df,\n",
    "    how='inner',\n",
    "    on='trace_id',\n",
    "    suffixes=('_anomaly', '_labels'),\n",
    "    validate=\"m:m\" # remind what was expected (yes, really m:m)\n",
    ").loc[lambda x: _filter_ano_time(x)]\n",
    "\n",
    "# Add \"period_type\" column based on conditions\n",
    "anomaly_df['period_type'] = np.where(\n",
    "    anomaly_df['time'].between(anomaly_df['ref_start'], anomaly_df['ref_end']), 'I_R',\n",
    "    np.where(anomaly_df['time'].between(anomaly_df['ano_start'], anomaly_df['ano_end']), 'I_A', \"Not In period\")\n",
    ")\n",
    "anomaly_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\n",
    "        'driver_BlockManager_memory_memUsed_MB_value',\n",
    "        'driver_jvm_heap_used_value',\n",
    "        'avg_jvm_heap_used_value',\n",
    "        'avg_executor_filesystem_hdfs_write_ops_value_1_diff',\n",
    "        'avg_executor_cpuTime_count_1_diff',\n",
    "        'avg_executor_runTime_count_1_diff',\n",
    "        'avg_executor_shuffleRecordsRead_count_1_diff',\n",
    "        'avg_executor_shuffleRecordsWritten_count_1_diff'\n",
    "    ]\n",
    "features_code_to_labels = {\n",
    "    f\"feature_{code}\": feature_name \n",
    "    for code, feature_name in enumerate(continuous_features)\n",
    "}\n",
    "features_label_to_code = {\n",
    "    value: key for key, value in features_code_to_labels.items()\n",
    "}\n",
    "features_code = features_code_to_labels.keys()\n",
    "print(features_code_to_labels.keys())\n",
    "print(features_label_to_code.keys())\n",
    "\n",
    "anomaly_df.rename(columns=features_label_to_code, inplace=True)\n",
    "anomaly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for trace_id \"1_1\" (bursty_input)\n",
    "bursty_input1_df = anomaly_df[anomaly_df[\"trace_id\"] == \"1_1\"]\n",
    "\n",
    "# Extract relevant columns\n",
    "columns_to_plot = bursty_input1_df.columns[\n",
    "    bursty_input1_df.columns.str.startswith(\"feature\")\n",
    "]\n",
    "\n",
    "# Set up subplot grid\n",
    "num_rows, num_cols = 4, 2\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 10), sharex=True)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot each column in the grid\n",
    "for ax, column in zip(axes.flatten(), columns_to_plot):\n",
    "    sns.lineplot(x='time', y=column, data=bursty_input1_df, ax=ax)\n",
    "    ax.set(title=column, xlabel='Time', ylabel='Value')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sufficient features space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input\n",
    "- Abnormal interval: $I_A$.\n",
    "- Reference interval $I_R$.\n",
    "\n",
    "Ouput\n",
    "- [ ] List of all the features occuring either during $I_A$ or $I_R$. \n",
    "  - [ ] Filter the data according to the time two intervals.\n",
    "  - [ ] Remove features that do not vary during this intervals / Select all the features varying during this time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---CODE HERE---- (UNUSED. NOT RELEVANT ANYMORE)\n",
    "def sufficient_features_space(data: pd.DataFrame, features: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter the relevant features based on the given data and return the list of features with sufficient variance.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataframe containing the data.\n",
    "        features (List[str]): The list of features to consider.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The list of features with sufficient variance.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If `data` is not a dataframe.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        relevant_data = df[(df[\"ref_start\"] <= df[\"time\"]) & (df[\"time\"] <= df[\"ref_end\"]) &\n",
    "                             (df[\"ano_start\"] <= df[\"time\"]) & (df[\"time\"] <= df[\"ano_end\"])]\n",
    "        relevant_data = relevant_data[features]\n",
    "        return [col for col in relevant_data.columns if relevant_data[col].var() > 1e-16]  # Adjust var threshold\n",
    "    else:\n",
    "        raise AttributeError(\"`data` is not a dataframe\")\n",
    "\n",
    "# Example usage with bursty_input1_df\n",
    "sufficient_features_space(bursty_input1_df, features_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsa_tsr_series_split(\n",
    "        df: pd.DataFrame,\n",
    "        trace_id: str,\n",
    "        ano_id: int\n",
    "        ) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Get annotated time series: TSA (Time Series Anomaly) and \n",
    "    TSR (Time Series Reference) of a given event, characterized by its \n",
    "    `trace_id` and its `ano_id`. \n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing time series data.\n",
    "    - trace_id (str): The identifier for the time series.\n",
    "    - ano_id (int): The identifier for the anomaly.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two DataFrames - TSA and TSR.\n",
    "    \"\"\"\n",
    "    tsa = df[(df['trace_id'] == trace_id)\n",
    "             & (df['ano_id'] == ano_id)\n",
    "             & (df['period_type'] == 'I_A')]\n",
    "    tsr = df[(df['trace_id'] == trace_id)\n",
    "             & (df['ano_id'] == ano_id)\n",
    "             & (df['period_type'] == 'I_R')]\n",
    "    \n",
    "    return tsa, tsr\n",
    "\n",
    "# Example usage:\n",
    "tsa, tsr = tsa_tsr_series_split(bursty_input1_df, trace_id=\"1_1\", ano_id=0)\n",
    "print(f\"TSA: Shape: {tsa.shape}. Observed period_type: {tsa['period_type'].unique()}.\")\n",
    "print(f\"TSR: Shape: {tsr.shape}. Observed period_type: {tsr['period_type'].unique()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsa[\"period_type\"].unique())\n",
    "tsa.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsr[\"period_type\"].unique())\n",
    "tsr.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single feature reward (section 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "* [x] Class Entropy\n",
    "* [x] Segmentation Entropy\n",
    "* [x] Penalization (for mixed segments)\n",
    "* [x] Normalization by feature size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class entropy\n",
    "\n",
    "**Input**\n",
    "* $TS_A$, $TS_R$ two time series from the abnormal and the reference intervals respectively.\n",
    "\n",
    "**Compute**\n",
    "\n",
    "* $|TS_A|$, $|TS_A|$: number of points in the two series.\n",
    "* The weight of each series (empirical probability that one point belong to that class).\n",
    "$$\n",
    "    p_A = \\frac{|TS_A|}{|TS_A| + |TS_R|}, \\quad p_R = \\frac{|TS_R|}{|TS_A| + |TS_R|}\n",
    "$$\n",
    "* The class entropy of the feature $f$:\n",
    "$$\n",
    "    H_{Class}(f) = p_A \\cdot \\log\\left(\\frac{1}{p_A}\\right) + p_R \\cdot \\log\\left(\\frac{1}{p_R}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---CODE HERE----\n",
    "def class_entropy(ts_anomaly: Iterable[float], ts_reference: Iterable[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the class entropy based on the number of instances in\n",
    "    TSA (True Samples for Anomaly) and TSR (True Samples for Regular).\n",
    "\n",
    "    Parameters:\n",
    "    - TSA (Iterable[float]): List or array containing instances\n",
    "        classified as True Anomalies.\n",
    "    - TSR (Iterable[float]): List or array containing instances\n",
    "        classified as True Regular.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated class entropy.\n",
    "    \"\"\"\n",
    "    p_A = len(ts_anomaly) / (len(ts_anomaly) + len(ts_reference))\n",
    "    p_R = len(ts_reference) / (len(ts_anomaly) + len(ts_reference))\n",
    "\n",
    "    H_Class_f = -(p_A * np.log2(p_A) + p_R * np.log2(p_R))\n",
    "    return H_Class_f\n",
    "\n",
    "# example with trace_id = 1_1 and ano_id = 1\n",
    "TSA, TSR = tsa.copy(), tsr.copy()\n",
    "class_entropy(TSA, TSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation entropy (???)\n",
    "\n",
    "**Input**\n",
    "* Time series $TS$ of a feature $f$: $(X_t)_{t=1,\\dots,T}$. \n",
    "* Anomaly time annotations: $A_S = (a_s)_{s=1,\\dots, S}  \\subset \\{1, \\dots, T\\}$  \n",
    "\n",
    "Example: In the sequence, AANNNNANNN. The anomaly time anotations would be (1,2,7). We are given only the different time intervals from which we have to derive the different anomaly time anotation and then the segment (this is just an heuristic, not necessarily the way it would be implemented).\n",
    "\n",
    "**Compute**\n",
    "* The segments. A segment is a (contiguous) sequence of point with the same label (anomaly or not anomaly)\n",
    "* The segmentation entropy of the feature $f$:\n",
    "$$\n",
    "    H_{Segmentation}(f) = \\sum_{i=1}^{n} p_i \\cdot \\log\\left(\\frac{1}{p_i}\\right)\n",
    "$$\n",
    "\n",
    "### Penalization for mixed segments\n",
    "\n",
    "$$\n",
    "    H^+_{Segmentation}(f) = H_{Segmentation}(f) + \\sum_{j=1}^{m} H_{Segmentation}(c_j^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_TS(df: pd.DataFrame, variable: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Generate a time series (TS) for a given variable based on unique combinations\n",
    "    of 'variable' and 'period_type' in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame containing the data.\n",
    "    - variable (str): The variable for which the time series is generated.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: The generated time series.\n",
    "    \"\"\"\n",
    "    # Use set to get unique combinations of 'variable' and 'period_type'\n",
    "    unique_values = df[[variable, 'period_type']].drop_duplicates()\n",
    "    \n",
    "    # Determine the period type for each unique value\n",
    "    TS = unique_values.groupby(variable)['period_type'].apply(\n",
    "        lambda x: 'I_M' if set(x) == {'I_R', 'I_A'} else x.iloc[0]\n",
    "    )\n",
    "    return TS\n",
    "\n",
    "def penalized_segmentation_entropy(TS: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the segmentation entropy with a penalty for mixed segments.\n",
    "\n",
    "    Parameters:\n",
    "    - TS (pd.Series): Time series data.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated segmentation entropy with mixed segment penalty.\n",
    "    \"\"\"\n",
    "    # Group the series by consecutive identical values and get segment lengths\n",
    "    segments = [(k, sum(1 for _ in g)) for k, g in groupby(TS)]\n",
    "    \n",
    "    # Filter segments for 'I_R' and 'I_A' only\n",
    "    relevant_segments = [(label, length) for label, length in segments\n",
    "                         if label in ['I_R', 'I_A']]\n",
    "    \n",
    "    # Calculate segment probabilities\n",
    "    total_length = sum(length for _, length in relevant_segments)\n",
    "    probabilities = [length / total_length for _, length in relevant_segments\n",
    "                     if total_length > 0]\n",
    "    \n",
    "    # Calculate segmentation entropy\n",
    "    H_Segmentation_f = -np.sum(np.fromiter(\n",
    "        (p * np.log2(p) for p in probabilities if p > 0), dtype=float)\n",
    "    )  # avoid log(0)\n",
    "\n",
    "    # Penalty for mixed segments\n",
    "    penalty = np.sum(\n",
    "        np.fromiter(\n",
    "            (\n",
    "                1/length * np.log2(1/length) for label, length in segments \n",
    "                if label == 'I_M' for _ in range(length)\n",
    "            ),\n",
    "            dtype=float)\n",
    "    )\n",
    "\n",
    "    return H_Segmentation_f - penalty  # Subtract the penalty\n",
    "\n",
    "feature = \"feature_0\"\n",
    "TS = generate_TS(pd.concat([TSA, TSR]), feature)\n",
    "print(TS.unique())\n",
    "print(f\"The segmentation entropy with mixed segment penalty for {feature}, is: {penalized_segmentation_entropy(TS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupby(TS): This groups the TS series by consecutive identical values. It returns keys (k) and groups (g). The key is the value around which the grouping is done (e.g., 'I_R', 'I_A', or 'I_M'), and the group is an iterator that you can loop over to get all items in the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization by feature size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ D(f) = \\frac{H_{Class}(f)}{H^+_{Segmentation}(f)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_based_reward(\n",
    "        class_entropy_value: float, \n",
    "        penalized_segmentation_entropy_value: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the entropy-based feature distance.\n",
    "\n",
    "    Args:\n",
    "    - class_entropy_value (float): The class entropy of the feature.\n",
    "    - penalized_segmentation_entropy_value (float): The regularized segmentation \n",
    "      entropy of the feature.\n",
    "\n",
    "    Returns:\n",
    "    float: The normalized entropy-based feature distance.\n",
    "    \"\"\"\n",
    "    return (class_entropy_value / penalized_segmentation_entropy_value\n",
    "            if penalized_segmentation_entropy_value != 0 else 0)\n",
    "\n",
    "Distance = entropy_based_reward(class_entropy(TSA, TSR), \n",
    "                                    penalized_segmentation_entropy(TS))\n",
    "print(f\"Feature Distance: {Distance}\")\n",
    "\n",
    "def single_feature_reward(df, feature, trace_id, ano_id):\n",
    "    \"\"\"\n",
    "    Calculate the reward for a single feature based on entropy.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input dataframe.\n",
    "    feature (str): The name of the feature.\n",
    "    trace_id (str): The trace ID column name.\n",
    "    ano_id (str): The anomaly ID column name.\n",
    "\n",
    "    Returns:\n",
    "    float: The entropy-based reward for the feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    ts_ano, ts_ref = tsa_tsr_series_split(df, trace_id=trace_id, ano_id=ano_id)\n",
    "    ts = generate_TS(pd.concat([ts_ano, ts_ref]), feature)\n",
    "    ts_class_entropy = class_entropy(tsa, tsr)\n",
    "    ts_segment_entropy = penalized_segmentation_entropy(ts)\n",
    "    entropy_based_reward(ts_class_entropy, ts_segment_entropy)\n",
    "\n",
    "    return entropy_based_reward(ts_class_entropy, ts_segment_entropy)\n",
    "\n",
    "\n",
    "feature = \"feature_0\"\n",
    "reward = single_feature_reward(\n",
    "    bursty_input1_df, feature=feature, trace_id=\"1_1\", ano_id=0\n",
    ")\n",
    "print(f\"The single-feature reward of '{feature}': {reward}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing explanations (section 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "5.1 Step 1: reward leap filtering The single-feature distance function produces a ranking of\n",
    "all features based on their individual rewards. Sharp changesin the reward between successive features in the ranking indicate a semantic change: Features that rank below a sharp drop in the reward are unlikely to contribute to an explanation. Therefore, features whose distance is low, relatively to\n",
    "other features, can be safely discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with more complex segmentation have lower rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_rewards(df: pd.DataFrame, trace_id: str, ano_id: int, features_code: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes feature rewards for continuous features.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the features.\n",
    "    - trace_id (str): The trace ID.\n",
    "    - ano_id (int): The anomaly ID.\n",
    "    - features_code (List[str]): List of feature names.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing feature names and their corresponding rewards.\n",
    "    \"\"\"\n",
    "    feature_rewards = {\n",
    "        feature: single_feature_reward(\n",
    "            df, feature=feature, trace_id=trace_id, ano_id=ano_id\n",
    "        )\n",
    "        for feature in features_code\n",
    "    }\n",
    "    return feature_rewards\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_feature_on_rewards(df, trace_id, ano_id, features_code, ignore_first=False, plot=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Plots the feature rewards for continuous features and identifies the elbow point.\n",
    "    Optionally ignores the first sharpest drop if it is the first feature.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the features.\n",
    "    - trace_id (str): The trace ID.\n",
    "    - ano_id (int): The anomaly ID.\n",
    "    - features_code (List[str]): List of feature names.\n",
    "    - ignore_first (bool, optional): Whether to ignore the first sharpest drop. Defaults to False.\n",
    "    - plot (bool, optional): Whether to plot the feature rewards. Defaults to True.\n",
    "    - verbose (bool, optional): Whether to print detailed information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: The significant features identified up to the elbow point.\n",
    "    \"\"\"\n",
    "    feature_rewards = compute_feature_rewards(\n",
    "        df, trace_id=trace_id, ano_id=ano_id, features_code=features_code\n",
    "        )\n",
    "    sorted_features = sorted(feature_rewards.items(), key=lambda x: x[1], reverse=True)\n",
    "    variables, rewards = zip(*sorted_features)\n",
    "\n",
    "    # Calculate differences between successive feature rewards\n",
    "    differences = np.abs(np.diff(rewards))\n",
    "    elbow_point = np.argmax(differences)\n",
    "\n",
    "    # Check if the elbow point is the first feature and whether to ignore it\n",
    "    if ignore_first and elbow_point == 0:\n",
    "        second_largest_drop = np.argmax(differences[1:]) + 1  # Offset by 1 due to slicing\n",
    "        elbow_point = second_largest_drop\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(variables, rewards, marker='o', linestyle='--', color='b')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Feature Reward')\n",
    "        plt.title('Feature Rewards for Continuous Features')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.axvline(x=variables[elbow_point], color='r', linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "    significant_features = list(variables[:elbow_point + 1])  # Include elbow_point\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Significant features:\", significant_features)\n",
    "\n",
    "    return significant_features\n",
    "\n",
    "# Example usage\n",
    "significant_features_1_1_1 = select_feature_on_rewards(bursty_input1_df, \"1_1\", 1, features_code, ignore_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gotcha, I told you guy, \"feature_0\" is not returned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: false positive filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: filtering by correlation clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trace_anomaly(\n",
    "    df: pd.DataFrame,\n",
    "    trace_id: str,\n",
    "    ano_id: int,\n",
    "    selected_features: List[str],\n",
    "    plot_correlation: bool = False,\n",
    "    threshold: float = 0.8,\n",
    "    linkage: str = 'complete',\n",
    "    verbose: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Process the trace anomaly by performing the following steps:\n",
    "    1. Retrieve the TSA (Trace Start Anomaly) and TSR (Trace Stop Anomaly) data\n",
    "    for the given trace_id and ano_id.\n",
    "    2. Concatenate the TSA and TSR data and select the specified features.\n",
    "    3. Calculate the correlation matrix of the selected features.\n",
    "    4. Cluster the strongly correlated features using agglomerative clustering.\n",
    "    5. Identify the representative features for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the features.\n",
    "    - trace_id (str): The ID of the trace.\n",
    "    - ano_id (int): The ID of the anomaly.\n",
    "    - selected_features (List[str]): The list of selected features.\n",
    "    - threshold (float, optional): The threshold for identifying strong\n",
    "      correlations. Defaults to 0.8.\n",
    "    - linkage (str, optional): The linkage method for agglomerative clustering.\n",
    "      Defaults to 'complete'.\n",
    "\n",
    "    Returns:\n",
    "    - representative_features (List[str]): The list of representative features\n",
    "      for each cluster.\n",
    "\n",
    "    #TODO: Idea for False Positive Filtering (why don't we simply add time in \n",
    "    #   agglomerative clustering)  Then, we remove all features in the same \n",
    "    #   cluster than him. \n",
    "    # Pay attention to exclude time when looking for best \"single_feature_reward\"\n",
    "    #   in its cluster.\n",
    "    # ==> This is equivalent to simply drop the cluster containing afterward.\n",
    "    \"\"\"\n",
    "    # Check that only one feature is selected\n",
    "    if len(selected_features) == 1:\n",
    "        if verbose:\n",
    "            print(f\"Only one feature selected: {selected_features[0]}\")\n",
    "        return selected_features\n",
    "    # Retrieve the TSA (Trace Start Anomaly) and TSR (Trace Stop Anomaly) data\n",
    "    # for the given trace_id and ano_id\n",
    "    tsa, tsr = tsa_tsr_series_split(df, trace_id=trace_id, ano_id=ano_id)\n",
    "\n",
    "    # Concatenate the TSA and TSR data and select the specified features\n",
    "    data: pd.DataFrame = pd.concat([tsa, tsr])[selected_features]\n",
    "\n",
    "    # Calculate the correlation matrix of the selected features\n",
    "    corr_matrix = data.corr().abs()\n",
    "    # Create a mask to hide the upper triangle of the correlation matrix for aesthetics.\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "    if plot_correlation:\n",
    "        # Create a heatmap to visualize the correlation matrix without redundancy\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\",\n",
    "                    cmap='coolwarm', square=True)\n",
    "        plt.title('Correlation Matrix without Redundancy Heatmap')\n",
    "        plt.show()\n",
    "\n",
    "    # Cluster the strongly correlated features using agglomerative clustering.\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None, affinity='precomputed', linkage=linkage,\n",
    "        distance_threshold=threshold)\n",
    "    clustering.fit(1 - corr_matrix)  # 1 - corr_matrix b/c agglomerative clustering uses distances\n",
    "\n",
    "    # Identify the representative features for each cluster.\n",
    "    cluster_labels = clustering.labels_\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    representative_features = []\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        # Select the features in the cluster.\n",
    "        indices = np.where(cluster_labels == cluster)[0] # feature indices\n",
    "        features_in_cluster = corr_matrix.columns[indices] # feature names\n",
    "        if \"time\" in features_in_cluster: # just in case someone include \"time\" by mistake.\n",
    "            features_in_cluster.drop(\"time\")\n",
    "\n",
    "        # Calculate single feature reward for each feature in the cluster.\n",
    "        feature_rewards = {\n",
    "            feature: single_feature_reward(df, feature, trace_id, ano_id)\n",
    "            for feature in features_in_cluster\n",
    "        }\n",
    "        # Select the feature with the highest reward.\n",
    "        selected_feature = max(\n",
    "            feature_rewards, key=feature_rewards.get\n",
    "        )\n",
    "        representative_features.append(selected_feature)\n",
    "\n",
    "    # print the representative features and associated features for each cluster\n",
    "    for cluster in unique_clusters:\n",
    "        indices = np.where(cluster_labels == cluster)[0]\n",
    "        if verbose:\n",
    "            print(f\"Cluster {cluster}:\", \", \".join(data.columns[indices]))\n",
    "            print(f\"Representative features: {representative_features[cluster]}\\n\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Selected representative characteristics: \",\n",
    "              representative_features)\n",
    "\n",
    "    return representative_features\n",
    "\n",
    "\n",
    "# Example\n",
    "representative_features_1_1_1 = process_trace_anomaly(\n",
    "    bursty_input1_df, '1_1', 1, significant_features_1_1_1, plot_correlation=True,\n",
    "    threshold=0.8, linkage='complete', verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Building final explanations (for human reading and aesthetics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = 1 2 4 7 10\n",
    "cumsum = 1 3 7 14 24\n",
    "inverse_cumsum = 24 14 7 3 1\n",
    "range(1, len(data + 1)) = 1 2 3 4 5\n",
    "data/cumsum = 1 3/2 7/3 14/4 24/5 (1 1.5 2.33 3.5 4.75)\n",
    "data/inverse_cumsum 1/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def find_optimal_threshold(df, trace_id, ano_id, feature, bins=10):\n",
    "#     # Split the time series into normal and abnormal data\n",
    "#     tsa, tsr = tsa_tsr_series_split(df, trace_id=trace_id, ano_id=ano_id)\n",
    "\n",
    "#     # Combine reference and abnormal data\n",
    "#     combined_data = pd.concat([tsa[feature], tsr[feature]]).sort_values().reset_index(drop=True)\n",
    "\n",
    "#     # Calculate the cumulative sum of the sorted data\n",
    "#     cumulative_sum = np.cumsum(combined_data)\n",
    "\n",
    "#     # Calculate the cumulative sum in reverse order\n",
    "#     cumulative_sum_reverse = np.cumsum(combined_data.iloc[::-1]).iloc[::-1]\n",
    "\n",
    "#     # Calculate the information gain for each split point\n",
    "#     information_gain = -calculate_entropy(combined_data) - calculate_entropy(cumulative_sum / np.arange(1, len(combined_data) + 1))\n",
    "#     information_gain -= calculate_entropy(cumulative_sum_reverse / np.arange(1, len(combined_data) + 1))\n",
    "\n",
    "#     # Find the index of the maximum information gain\n",
    "#     optimal_index = np.argmax(information_gain)\n",
    "\n",
    "#     # Return the optimal threshold and corresponding information gain\n",
    "#     return combined_data[optimal_index] #, information_gain[optimal_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: HAS TO BE IMPROVED\n",
    "def calculate_entropy(data, bins=10):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a distribution of data.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): The input data.\n",
    "\n",
    "    Returns:\n",
    "    float: The entropy value.\n",
    "    \"\"\"\n",
    "    hist, _ = np.histogram(data, bins=bins)\n",
    "\n",
    "    return entropy(hist)\n",
    "\n",
    "\n",
    "def find_optimal_threshold(df, trace_id, ano_id, feature, bins=10):\n",
    "    tsa, tsr = tsa_tsr_series_split(df, trace_id=trace_id, ano_id=ano_id)\n",
    "    combined_data = pd.concat([tsa[feature], tsr[feature]]).sort_values()\n",
    "\n",
    "    # Initialiser le seuil optimal et le gain d'information maximal\n",
    "    optimal_threshold = None\n",
    "    max_information_gain = -np.inf\n",
    "\n",
    "    # Use binary search over the potential split points\n",
    "    low = 0\n",
    "    high = len(combined_data) - 1\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        threshold = combined_data.iloc[mid]\n",
    "        lower_entropy = calculate_entropy(combined_data[combined_data <= threshold], bins=bins)\n",
    "        upper_entropy = calculate_entropy(combined_data[combined_data > threshold], bins=bins)\n",
    "        information_gain = - lower_entropy - upper_entropy\n",
    "\n",
    "        # Update the optimal threshold if the information gain is higher\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "        # If the information gain is the same on both sides, we can stop\n",
    "        if information_gain == max_information_gain:\n",
    "            break\n",
    "        elif information_gain < max_information_gain:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            low = mid + 1\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "def find_abnormal_intervals(df, trace_id, ano_id, feature, bins=10):\n",
    "    optimal_threshold = find_optimal_threshold(df, trace_id, ano_id, feature, bins=bins)\n",
    "    return [(None, optimal_threshold)]  # Retourne un intervalle sous la forme (min, seuil)\n",
    "\n",
    "def feature_explanation_predicate(df, trace_id, ano_id, feature, bins=10):\n",
    "    # Trouver les intervalles anormaux pour la caractéristique\n",
    "    abnormal_intervals = find_abnormal_intervals(df, trace_id, ano_id, feature, bins=bins)\n",
    "\n",
    "    # Construire les prédicats pour la caractéristique\n",
    "    predicates = []\n",
    "    for interval in abnormal_intervals:\n",
    "        if interval[0] is None:  # Seuil supérieur seulement\n",
    "            predicates.append(f\"{feature} ≤ {interval[1]}\")\n",
    "        elif interval[1] is None:  # Seuil inférieur seulement\n",
    "            predicates.append(f\"{feature} ≥ {interval[0]}\")\n",
    "        else:  # Intervalle complet\n",
    "            predicates.append(f\"({feature} ≥ {interval[0]} ∧ {feature} ≤ {interval[1]})\")\n",
    "\n",
    "    # Joindre les prédicats par \"∨\" (disjonction)\n",
    "    return \" ∨ \".join(predicates)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# tsa, tsr = tsa_tsr_series_split(bursty_input1_df, trace_id=\"1_1\", ano_id=1)\n",
    "# TSA, TSR = tsa.copy(), tsr.copy()\n",
    "\n",
    "feature = \"feature_0\"\n",
    "optimal_threshold = find_optimal_threshold(\n",
    "    df=bursty_input1_df, trace_id=\"1_1\", ano_id=0, feature=feature)\n",
    "print(\"Seuil optimal :\", optimal_threshold)\n",
    "\n",
    "feature_explanation_predicate(df=bursty_input1_df, trace_id=\"1_1\", ano_id=0, feature=feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_explanation(\n",
    "        df,\n",
    "        trace_id,\n",
    "        ano_id,\n",
    "        features,\n",
    "        plot_rewards=False,\n",
    "        plot_correlation=False,\n",
    "        threshold=0.8,\n",
    "        linkage='complete', \n",
    "        ignore_first : bool = False,\n",
    "        verbose: bool = False,\n",
    "        cluster: bool = True):\n",
    "    # Step 1: Reward leap filtering (select features based on reward)\n",
    "    selected_features = select_feature_on_rewards(\n",
    "        df, \n",
    "        trace_id=trace_id,\n",
    "        ano_id=ano_id,\n",
    "        features_code=features,\n",
    "        plot=plot_rewards,\n",
    "        verbose=verbose,\n",
    "        ignore_first=ignore_first,\n",
    "\n",
    "    )\n",
    "\n",
    "    # Step 2: False Positive Filtering (#TODO: Coming soon...)\n",
    "\n",
    "    selected_features_without_fp = selected_features.copy()\n",
    "    if cluster == False:\n",
    "        return selected_features_without_fp\n",
    "    # Step 3: Filtering by correlation clustering\n",
    "    representative_features = process_trace_anomaly(df, \n",
    "                                                    trace_id=trace_id, \n",
    "                                                    ano_id=ano_id, \n",
    "                                                    selected_features=selected_features, \n",
    "                                                    plot_correlation=plot_correlation, \n",
    "                                                    threshold=threshold, \n",
    "                                                    linkage=linkage, \n",
    "                                                    verbose=verbose)\n",
    "    \n",
    "    return representative_features\n",
    "\n",
    "build_explanation(\n",
    "    bursty_input1_df, \n",
    "    trace_id=\"1_1\", \n",
    "    ano_id=1, \n",
    "    features=features_code, \n",
    "    plot_rewards=False, \n",
    "    plot_correlation=False,\n",
    "    verbose=False,\n",
    "    ignore_first=False,\n",
    "    cluster=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def select_features_using_decision_tree(df, features_code, plot=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Selects significant features using a decision tree and identifies the elbow point\n",
    "    for feature importances.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the features and target variable.\n",
    "    - features_code (List[str]): List of feature names.\n",
    "    - plot (bool, optional): Whether to plot the feature importances. Defaults to True.\n",
    "    - verbose (bool, optional): Whether to print detailed information. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: The significant features identified up to the elbow point.\n",
    "    \"\"\"\n",
    "    # Normalisation des données avec StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X = df[features_code]\n",
    "    y = df['period_type']  # Assuming 'period_type' is the target variable\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Créer et entraîner l'arbre de décision\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    # Obtenir l'importance des caractéristiques\n",
    "    feature_importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "\n",
    "    # Calculer les différences entre les importances des caractéristiques successives\n",
    "    feature_importances_sorted = feature_importances.sort_values(ascending=False)\n",
    "    differences = np.abs(np.diff(feature_importances_sorted))\n",
    "    elbow_point = np.argmax(differences)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(feature_importances_sorted.index, feature_importances_sorted, marker='o', linestyle='--', color='b')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Feature Importance')\n",
    "        plt.title('Feature Importances for Decision Tree')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.axvline(x=feature_importances_sorted.index[elbow_point], color='r', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    significant_features = list(feature_importances_sorted.index[:elbow_point + 1])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Significant features:\", significant_features)\n",
    "\n",
    "    return significant_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "tsa, tsr = tsa_tsr_series_split(anomaly_df[anomaly_df[\"trace_id\"] == '1_1'], trace_id=\"1_1\", ano_id=1)\n",
    "ts = pd.concat([tsa, tsr])\n",
    "df = ts.copy()\n",
    "significant_features_dt = select_features_using_decision_tree(df, continuous_variables, plot=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "instability(df, trace_id, ano_id, features, repeat=5, sample_frac=0.8, replace=False, random_state=42):\n",
    "    # INPUTS:\n",
    "    #   df: DataFrame containing the data\n",
    "    #   trace_id: Trace ID for the anomaly\n",
    "    #   ano_id: Anomaly ID\n",
    "    #   features: List of features to analyze\n",
    "    #   repeat: Number of times to repeat the process for stability estimation (default is 5)\n",
    "    #   sample_frac: Fraction of the DataFrame to use for each sample during instability calculation (default is 0.8)\n",
    "    #   replace: Whether to sample with replacement (default is False)\n",
    "    #   random_state: Random seed for reproducibility (default is 42)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    Set random seed to random_state\n",
    "\n",
    "    # Filter the DataFrame to the specified anomaly\n",
    "    single_ano_df = Filter rows in df where trace_id matches trace_id and ano_id matches ano_id\n",
    "\n",
    "    # Initialize an empty list to store explanatory features\n",
    "    all_explanatory_features = []\n",
    "\n",
    "    # Repeat the following steps 'repeat' times\n",
    "    For each _ in range(repeat):\n",
    "        # Randomly subsample 80% of the dataframe\n",
    "        sample_df = Randomly sample sample_frac fraction of rows from single_ano_df with or without replacement\n",
    "\n",
    "        # Build an explanation using the subsample\n",
    "        explanatory_features = build_explanation(sample_df, trace_id, ano_id, features)\n",
    "\n",
    "        # Add the explanation features to the list\n",
    "        Append explanatory_features to all_explanatory_features\n",
    "\n",
    "    # Compute the frequency of each feature in the list\n",
    "    feature_counts = Count occurrences of each feature in all_explanatory_features\n",
    "\n",
    "    # Compute consistency as entropy using the feature frequencies\n",
    "    consistency = Calculate entropy using feature_counts\n",
    "\n",
    "    # Return the computed consistency\n",
    "    Return consistency\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instability(\n",
    "    df: pd.DataFrame,\n",
    "    trace_id: str,\n",
    "    ano_id: int,\n",
    "    features: List[str],\n",
    "    plot_rewards: bool = False,\n",
    "    plot_correlation: bool = False,\n",
    "    verbose: bool = False,\n",
    "    threshold: float = 0.8,\n",
    "    linkage: str = 'complete',\n",
    "    repeat: int = 5,\n",
    "    sample_frac: int = 0.8,\n",
    "    replace: bool = False,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the instability (consistency as entropy) of feature explanations for a specific anomaly in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        trace_id (str): The trace ID for the anomaly.\n",
    "        ano_id (int): The anomaly ID.\n",
    "        features (List[str]): The list of features to analyze.\n",
    "        repeat (int): Number of times to repeat the process for stability estimation.\n",
    "        sample_frac (float): Fraction of the DataFrame to use for each sample during instability calculation.\n",
    "        replace (bool): Whether to sample with replacement.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        float: Instability (consistency as entropy) of feature explanations.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state) # for reproducibility\n",
    "\n",
    "    # 1. Filter the DataFrame to the specified anomaly\n",
    "    single_ano_df = df[(df[\"trace_id\"] == trace_id) & (df[\"ano_id\"] == ano_id)]\n",
    "    \n",
    "    # 2. Get all the set of explanation features.\n",
    "    all_explanatory_features = [\n",
    "        build_explanation(\n",
    "            single_ano_df.sample(frac=sample_frac, replace=replace),\n",
    "            trace_id=trace_id,\n",
    "            ano_id=ano_id,\n",
    "            features=features,\n",
    "            plot_rewards=plot_rewards,\n",
    "            plot_correlation=plot_correlation,\n",
    "            threshold=threshold,\n",
    "            linkage=linkage,\n",
    "            verbose=verbose\n",
    "        ) for _ in range(repeat)\n",
    "    ]\n",
    "\n",
    "    # 3. Compute the frequency for each feature in the list.\n",
    "    feature_counts = pd.Series(\n",
    "        np.concatenate(all_explanatory_features)).value_counts(normalize=True)\n",
    "\n",
    "    # 4. Compute consistency as entropy using the previous frequency.\n",
    "    consistency = -np.sum(feature_counts * np.log2(feature_counts))\n",
    "\n",
    "    return consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instability(bursty_input1_df, trace_id=\"1_1\", ano_id=0, features=features_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instability for decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instability_decision_tree(df, trace_id, ano_id, features, repeat=5, sample_frac=0.8, replace=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate the instability of feature selection using a decision tree.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        trace_id (str): The trace ID for the anomaly.\n",
    "        ano_id (int): The anomaly ID.\n",
    "        features (List[str]): The list of features to analyze.\n",
    "        repeat (int): Number of times to repeat the process.\n",
    "        sample_frac (float): Fraction of the DataFrame to use for each sample.\n",
    "        replace (bool): Whether to sample with replacement.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        float: Instability of feature selection.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    all_selected_features = []\n",
    "    for _ in range(repeat):\n",
    "        sampled_df = df.sample(frac=sample_frac, replace=replace)\n",
    "        X_sampled = sampled_df[features]\n",
    "        y_sampled = sampled_df['period_type']\n",
    "\n",
    "        # Normalisation des données\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_sampled)\n",
    "\n",
    "        # Entraînement de l'arbre de décision\n",
    "        clf = DecisionTreeClassifier(random_state=random_state)\n",
    "        clf.fit(X_scaled, y_sampled)\n",
    "\n",
    "        # Sélection des caractéristiques significatives\n",
    "        significant_features = [feature for feature, importance in zip(features, clf.feature_importances_) if importance > 0]\n",
    "        all_selected_features.extend(significant_features)\n",
    "\n",
    "    # Calcul de l'instabilité\n",
    "    feature_counts = pd.Series(all_selected_features).value_counts(normalize=True)\n",
    "    instability = -np.sum(feature_counts * np.log2(feature_counts))\n",
    "\n",
    "    return instability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "tsa, tsr = tsa_tsr_series_split(bursty_input1_df, trace_id=\"1_1\", ano_id=1)\n",
    "ts = pd.concat([tsa, tsr])\n",
    "df = ts.copy()\n",
    "instability_value = instability_decision_tree(df, \"1_1\", 1, features_code, repeat=5, sample_frac=0.8, replace=False, random_state=42)\n",
    "print(\"Instability:\", instability_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of outputs : folder_1_results.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output  : a CSV file with as columns:\n",
    "\n",
    "- trace_id\n",
    "- ano_id\n",
    "- exp_size\n",
    "- exp_instability, which is computed by subsampling 80% of the data (from both the reference and anomalous periods) to generate an explanation and repeating this 5 times to compute the instability metric. \n",
    "- explanation (using feature integer indices, not the names, to be shorter in display).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que results_df contienne les résultats d'EXstream\n",
    "results_list = []\n",
    "anomalies = labels_df[['trace_id', 'ano_id']].values.tolist()\n",
    "\n",
    "for trace_id, ano_id in anomalies:\n",
    "\n",
    "    # Utiliser l'arbre de décision pour obtenir les caractéristiques significatives\n",
    "    tsa, tsr = tsa_tsr_series_split(anomaly_df[anomaly_df[\"trace_id\"] == trace_id], trace_id=trace_id, ano_id=ano_id)\n",
    "    ts = pd.concat([tsa, tsr])\n",
    "    df = ts.copy()\n",
    "    significant_features_dt = select_features_using_decision_tree(df, continuous_variables, plot=False, verbose=False)\n",
    "\n",
    "\n",
    "    # Générer l'explication par extream sans cluster\n",
    "    explanation_features_extream = build_explanation(\n",
    "        anomaly_df[anomaly_df[\"trace_id\"] == trace_id], \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code,\n",
    "        plot_rewards=False, \n",
    "        plot_correlation=False,\n",
    "        verbose=False,\n",
    "        ignore_first=True,\n",
    "        cluster=False\n",
    "    )\n",
    "\n",
    "    # Générer l'explication par extream-cluster\n",
    "    explanation_features_extream_cluster = build_explanation(\n",
    "        anomaly_df[anomaly_df[\"trace_id\"] == trace_id], \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code,\n",
    "        plot_rewards=False, \n",
    "        plot_correlation=False,\n",
    "        verbose=False,\n",
    "        ignore_first=True,\n",
    "        cluster=True\n",
    "    )\n",
    "    \n",
    "    # Ajouter les résultats à la liste\n",
    "    results_list.append({\n",
    "        'trace_id': trace_id,\n",
    "        'ano_id': ano_id,\n",
    "        'conciseness_exstream': len(explanation_features_extream),\n",
    "        'conciseness_exstream_cluster': len(explanation_features_extream_cluster),\n",
    "        'conciseness_dt': len(significant_features_dt)\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame pour comparer les résultats\n",
    "comparison_df = pd.DataFrame(results_list)\n",
    "print(comparison_df)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Création du graphique\n",
    "ax = comparison_df.plot.bar(x='trace_id', y=['conciseness_exstream','conciseness_exstream_cluster', 'conciseness_dt'], figsize=(12, 6), color=['skyblue', 'gray', 'salmon'])\n",
    "\n",
    "# Réglages esthétiques\n",
    "plt.xlabel('Trace ID', fontsize=12)\n",
    "plt.ylabel('Explanation Size', fontsize=12)\n",
    "plt.title('Comparison of Explanation Sizes', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "\n",
    "# Optimiser l'espacement et le layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#save figure\n",
    "plt.savefig('comparison_conciseness.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of anomalies\n",
    "anomalies = labels_df[['trace_id', 'ano_id']].values.tolist()\n",
    "\n",
    "# Liste pour stocker les dictionnaires de résultats\n",
    "results_list = []\n",
    "\n",
    "for trace_id, ano_id in anomalies:\n",
    "    # Générer l'explication\n",
    "    explanation_features = build_explanation(\n",
    "        anomaly_df[anomaly_df[\"trace_id\"] == trace_id], \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code,\n",
    "        plot_rewards=False, \n",
    "        plot_correlation=False,\n",
    "        verbose=False,\n",
    "        ignore_first=True,\n",
    "        cluster=True\n",
    "    )\n",
    "    \n",
    "    # Calculer l'instabilité\n",
    "    exp_size = len(explanation_features)\n",
    "    exp_instability = instability(\n",
    "        bursty_input1_df, \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code\n",
    "    )\n",
    "    \n",
    "    # Ajouter les résultats à la liste\n",
    "    results_list.append({\n",
    "        'trace_id': trace_id,\n",
    "        'ano_id': ano_id,\n",
    "        'exp_size': exp_size,\n",
    "        'exp_instability': exp_instability,\n",
    "        'explanation': ', '.join(explanation_features)\n",
    "    })\n",
    "\n",
    "# Créer DataFrame à partir de la liste de dictionnaires\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Exporter en CSV\n",
    "results_df.to_csv('folder_1_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les résultats combinés\n",
    "combined_results_list = []\n",
    "anomalies = labels_df[['trace_id', 'ano_id']].values.tolist()\n",
    "\n",
    "for trace_id, ano_id in anomalies:\n",
    "    # Utiliser l'arbre de décision pour obtenir les caractéristiques significatives\n",
    "    tsa, tsr = tsa_tsr_series_split(anomaly_df[anomaly_df[\"trace_id\"] == trace_id], trace_id=trace_id, ano_id=ano_id)\n",
    "    ts = pd.concat([tsa, tsr])\n",
    "    df = ts.copy()\n",
    "    significant_features_dt = select_features_using_decision_tree(df, continuous_variables, plot=False, verbose=False)\n",
    "\n",
    "    # Générer l'explication par EXstream avec et sans cluster\n",
    "    explanation_features_extream = build_explanation(\n",
    "        anomaly_df[anomaly_df[\"trace_id\"] == trace_id], \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code,\n",
    "        plot_rewards=False, \n",
    "        plot_correlation=False,\n",
    "        verbose=False,\n",
    "        ignore_first=True,\n",
    "        cluster=False\n",
    "    )\n",
    "    explanation_features_extream_cluster = build_explanation(\n",
    "        anomaly_df[anomaly_df[\"trace_id\"] == trace_id], \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code,\n",
    "        plot_rewards=False, \n",
    "        plot_correlation=False,\n",
    "        verbose=False,\n",
    "        ignore_first=True,\n",
    "        cluster=True\n",
    "    )\n",
    "    \n",
    "    # Calculer l'instabilité pour EXstream avec cluster\n",
    "    exp_instability = instability(\n",
    "        bursty_input1_df, \n",
    "        trace_id=trace_id, \n",
    "        ano_id=ano_id, \n",
    "        features=features_code\n",
    "    )\n",
    "\n",
    "    # Ajouter les résultats combinés à la liste\n",
    "    combined_results_list.append({\n",
    "        'trace_id': trace_id,\n",
    "        'ano_id': ano_id,\n",
    "        'conciseness_exstream': len(explanation_features_extream),\n",
    "        'conciseness_exstream_cluster': len(explanation_features_extream_cluster),\n",
    "        'conciseness_dt': len(significant_features_dt),\n",
    "        'exp_instability': exp_instability\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame pour les résultats combinés\n",
    "combined_results_df = pd.DataFrame(combined_results_list)\n",
    "\n",
    "# Création du graphique de comparaison\n",
    "ax = combined_results_df.plot.bar(x='trace_id', y=['conciseness_exstream', 'conciseness_exstream_cluster', 'conciseness_dt'], figsize=(12, 6), color=['skyblue', 'gray', 'salmon'])\n",
    "plt.xlabel('Trace ID', fontsize=12)\n",
    "plt.ylabel('Explanation Size', fontsize=12)\n",
    "plt.title('Comparison of Explanation Sizes', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_conciseness.png', dpi=300) #save figure\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Exporter les résultats combinés en CSV\n",
    "combined_results_df.to_csv('combined_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
